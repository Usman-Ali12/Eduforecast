# -*- coding: utf-8 -*-
"""train_model.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k9CZRTZL9i2ItgcEYzVwzoj487NA5nKC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Load dataset
df = pd.read_csv('dmodel.csv')  # Make sure this file is in the same directory

# Drop unneeded columns
df = df.drop(['student_id'], axis=1)

# One-hot encode categorical columns
df_encoded = pd.get_dummies(df, columns=[
    'institution_name', 'program', 'gender', 'province', 'dropout_reason'
])

# Separate features and label
X = df_encoded.drop('dropout_status', axis=1)
y = df_encoded['dropout_status']

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Plot top 10 feature importances
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
features = X.columns

plt.figure(figsize=(10, 6))
plt.title("Top 10 Feature Importances")
plt.bar(range(10), importances[indices[:10]], align='center')
plt.xticks(range(10), [features[i] for i in indices[:10]], rotation=45)
plt.tight_layout()
plt.show()

# Save the model and columns
joblib.dump(model, 'dropout_model.pkl')
joblib.dump(list(X.columns), 'model_columns.pkl')

print("Model and columns saved.")

